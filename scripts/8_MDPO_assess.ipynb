{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess MDPO Results\n",
    "\n",
    "Loads MDPO optimization pickle files and evaluates training dynamics, convergence, and resultant device trajectories.\n",
    "Also equipped to perform SEPIO classification assessments on device placements to estimate achievable information throughput and classification accuracy limits.\n",
    "\n",
    "Overview:\n",
    "- MDPO (Multi-Device Placement Optimization) produces candidate device trajectories via genetic / annealing / gradient strategies.\n",
    "- This notebook ingests one or more MDPO result pickle files, extracts best solutions per generation, and summarizes fitness progression.\n",
    "- Rebuilds sensing voltage matrices for each device set to enable downstream SEPIO Monte Carlo classification benchmarking.\n",
    "\n",
    "Primary Inputs:\n",
    "- Pickle files containing MDPO run dictionaries (see key list in loading section).\n",
    "- Brain / ROI surface .mat files matching those used during optimization.\n",
    "- Lead field (.npz) files defining sensor-specific field vectors.\n",
    "\n",
    "Primary Outputs:\n",
    "- Plots: fitness vs device count, fitness vs generations/temperature, accuracy vs sensor count, sensor-by-device contour.\n",
    "- Arrays: voltage tensors per device set suitable for SEPIO training.\n",
    "- Summaries: Information Capacity (IC) and voltage statistics, distribution analyses by distance.\n",
    "\n",
    "Workflow Steps:\n",
    "1. Configure anatomical and lead field resources (matching optimization settings).\n",
    "2. Load MDPO pickle(s) and inspect key/value structure.\n",
    "3. Extract and optionally convert angles (radians vs degrees) for device trajectories.\n",
    "4. Reconstruct per-source voltages for each optimized device set.\n",
    "5. Compute IC / SNR summaries and voltage distributions.\n",
    "6. Run SEPIO Monte Carlo classification across sensor subsets.\n",
    "7. Visualize accuracy scaling and device/sensor trade-offs.\n",
    "\n",
    "Notes:\n",
    "- Ensure all anatomical and lead field settings match those used during original MDPO optimization or results will be invalid.\n",
    "- SEPIO training treats voltages as feature space; spatial accuracy can optionally be computed if implemented.\n",
    "- Information Capacity calculation assumes global `bandwidth` and `noise` constants consistent with device modality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports and Files ###\n",
    "# Core scientific / IO libraries\n",
    "import matplotlib.pyplot as plt   # Plotting utilities for fitness, accuracy, distributions\n",
    "import numpy as np                # Numerical arrays and vectorized operations\n",
    "from os import path               # Path joins (portable path construction)\n",
    "import sys                        # Modify import search path for local modules\n",
    "import pickle                     # Load MDPO optimization result dictionaries\n",
    "\n",
    "# Project module path insertion (two levels up to root for `modules/`)\n",
    "sys.path.insert(0, path.join('../..'))\n",
    "\n",
    "# Local project modules\n",
    "from modules.leadfield_importer import FieldImporter  # Loads lead field (.npz) arrays\n",
    "from scipy.io import loadmat                          # Reads MATLAB .mat surface/ROI files\n",
    "from modules.SEPIO import sepio                       # SEPIO classification / MC training routines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides graphing of MDPO results and performs SEPIO on desired sensor sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding device locations\n",
    "Settings __MUST__ match those chosen prior in optimization!\n",
    "\n",
    "Can potentially switch devices __IF__ they are the same shape lead fields (excluding sensor axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brainsuite files / anatomical resource configuration\n",
    "# Provide full brain and ROI geometry sets identical to those used during MDPO optimization.\n",
    "# Each entry in brain_data / roi_data corresponds to a separate .mat surface file with\n",
    "# faces, vertices, normals accessible under specified struct keys.\n",
    "folder = r\"...\\SEPIO_dataset\" # Root dataset folder (UPDATE to actual absolute path before running)\n",
    "\n",
    "# --- Single ROI Demo (Broca's area) ---\n",
    "brain_data = [loadmat(path.join(folder,'MDPO_data','brain.mat'))]  # Full brain surface\n",
    "roi_data = [loadmat(path.join(folder,'MDPO_data','r_broca.mat'))]  # ROI surface subset\n",
    "roi_weights = [1]              # Uniform weight (can scale relative importance across multiple ROIs)\n",
    "roi_label = ['ans']            # MATLAB struct key in ROI files (e.g., 'ans','auditory_roi')\n",
    "brain_label = ['brain']        # MATLAB struct key in brain files (e.g., 'brain','ans')\n",
    "\n",
    "do_weights = False            # Toggle: apply ROI weights to voltage/SNR/IC metrics\n",
    "\n",
    "\"\"\"\n",
    "# --- Multi-Region Example (ACC + left/right DLPFC/FEF) ---\n",
    "# Uncomment and adjust if evaluating multi-region optimization results.\n",
    "roi_data = [loadmat(path.join(folder,'ACC_inner_cortex.mat')),            # ACC inner cortex\n",
    "            loadmat(path.join(folder,'L-DLPFC_FEF.mat')),                # Left DLPFC/FEF\n",
    "            loadmat(path.join(folder,'R-DLPFC_FEF.mat'))]                # Right DLPFC/FEF\n",
    "brain_data = [loadmat(path.join(folder,'L_inner_cortex.mat')),           # Left hemisphere inner cortex\n",
    "              loadmat(path.join(folder,'R_inner_cortex.mat'))]           # Right hemisphere inner cortex\n",
    "roi_weights = [1.,0.5,0.5]        # Weighted emphasis for subregions (ACC higher priority)\n",
    "roi_label = ['ans','ans','ans']   # Struct keys for ROI files\n",
    "brain_label = ['ans','ans']       # Struct keys for brain files\n",
    "\n",
    "do_weights = False               # Enable to scale metrics by roi_weights\n",
    "\"\"\"\n",
    "\n",
    "# Dipole / biophysical parameters\n",
    "# Magnitude: 0.5e-9 nAm typical physiologic; 20e-9 nAm for phantom simulations\n",
    "magnitude = 0.5e-9        # nAm dipole moment used for voltage synthesis\n",
    "dipole_base_length = 2.5  # mm approximate cortical column length (contextual reference)\n",
    "\n",
    "# Lead field selection (ensure match with MDPO run; sensor geometry must be identical)\n",
    "fields_files = path.join(folder,'leadfields',\"SEEG_8e_500um_1500pitch.npz\")  # SEEG example\n",
    "#fields_files = path.join(folder,'leadfields',\"DISC_30mm_p2-5Sm_MacChr.npz\") # DiSc example\n",
    "#fields_files = path.join(folder,'leadfields',\"ECoG_1mm_500umgrid.npz\")       # ECoG example\n",
    "\n",
    "# DiSc specific aggregation parameters (ignored for SEEG/ECoG)\n",
    "macros = False          # If True, reduce DiSc electrodes into virtual macro rings\n",
    "virtual_rings = 4       # Number of macro rings (must be one of {1,2,4,8})\n",
    "\n",
    "# Processing / sensing environment parameters\n",
    "sensor_div = 8          # Sensor granularity for SEPIO sweeps (must divide sensor_max evenly)\n",
    "                        # Recommended: 16 for DiSc; 2 or 4 for SEEG depending on electrode count\n",
    "scale = 0.5             # mm per voxel (lead field spatial resolution)\n",
    "cl_wd = 1.0             # mm clearance diameter (legacy; may be unused)\n",
    "noise = 4.1             # μV RMS noise; 4.1 DiSc | 2.7 SEEG | varies for ECoG by diameter\n",
    "bandwidth = 100         # Samples per second (used for information capacity)\n",
    "Montage = False         # If True, compute montage differences (max-min) rather than single optimal\n",
    "\n",
    "# Lead field loading\n",
    "field_importer = FieldImporter()\n",
    "field = field_importer.load(fields_files)\n",
    "num_electrodes = np.shape(field_importer.fields)[4]  # Total electrodes in selected lead field\n",
    "fields = field_importer.fields\n",
    "midpoint = fields.shape[0]//2  # Lead field cube midpoint index (X/Y assumed square)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic / Anneal / Gradient (MDPO) result file handling\n",
    "# 'files' should list MDPO pickle result filenames located under folder/MDPO_data.\n",
    "# Each pickle expected to contain a dictionary capturing optimization metadata and results.\n",
    "# See key inventory below for structure expectations.\n",
    "\n",
    "# GA file locations\n",
    "out = path.join(folder,\"outputs\")  # Output directory for plots / summaries\n",
    "\n",
    "# Auditory cortex experiment set (paper example)\n",
    "files = [r\"genetic-1.pkl\",r\"genetic-2.pkl\",r\"genetic-3.pkl\",r\"genetic-4.pkl\",r\"genetic-5.pkl\"]\n",
    "\n",
    "# Dual region ACC / DLPFC-FEF example set\n",
    "#files = [r\"ACC-1.pkl\",r\"ACC-gradient-2.pkl\",r\"ACC-gradient-3.pkl\",\n",
    "#         r\"ACC-gradient-4.pkl\",r\"ACC-gradient-5.pkl\"]\n",
    "\n",
    "# Assign file N (device counts). For multi-device optimization runs, user must ensure\n",
    "# N_file matches number of devices represented in each pickle (or derive automatically).\n",
    "N_file = np.arange(1,len(files)+1)  # Integer device counts per file (sequential assumption)\n",
    "pickle_files = []\n",
    "for f in files:\n",
    "    pickle_files.append(path.join(folder,'MDPO_data',f))\n",
    "\n",
    "# Load Pickle files into `data` list\n",
    "# Each element: dict with keys like 'best solution:', 'fitnesses', 'device types', etc.\n",
    "data = []\n",
    "for p in pickle_files:\n",
    "    with open(p,'rb') as f:\n",
    "        data.append(pickle.load(f))\n",
    "\n",
    "# Assess data dict structure (verbosity intended for exploratory inspection)\n",
    "''' KEYS (Representative)\n",
    "'type'                      : Optimization method descriptor ('genetic','anneal','gradient')\n",
    "'best solution:'            : Flat array of concatenated device poses [x,y,z,alpha,beta,gamma]*N\n",
    "'best solution fitness'     : Scalar fitness value (e.g., total information capacity)\n",
    "'fitnesses'                 : List of fitness values per generation / iteration\n",
    "'best solution over generations' : List tracking best-so-far solution states\n",
    "'mean fitnesses'            : Mean population fitness per generation (genetic)\n",
    "'std fitnesses'             : Std dev of population fitness per generation\n",
    "'device names' / 'device types'  : Metadata describing each device instance\n",
    "'field files'               : Lead field file references ensuring reproducibility\n",
    "'N per device'              : Sensor counts per device if heterogeneous\n",
    "'LF scale'                  : Spatial scale used (mm per voxel)\n",
    "'dev diameter' / 'dev depth': Physical device geometry parameters\n",
    "'dev backend'               : Recording backend identifier (if provided)\n",
    "'voltage scale'             : Scaling factor used inside optimization\n",
    "'dev bandwidth'             : Effective sampling bandwidth\n",
    "'depth limits' / 'angle limits' : Constraints applied during search\n",
    "'montaged'                  : Boolean indicating montage-enabled fitness evaluation\n",
    "'''\n",
    "\n",
    "best = []\n",
    "radians = False # Return radians or degrees; MUST be FALSE for SEPIO; MUST be TRUE for visualization plotting\n",
    "if radians:\n",
    "    print(\"!!! DISABLE `radians` to use SEPIO (expects degrees in original pickle) !!!\")\n",
    "\n",
    "print('Data Summary:')\n",
    "for i,f in enumerate(files):\n",
    "    print(\"N =\",N_file[i])\n",
    "    depth = sum(isinstance(v, dict) for v in data[i].values()) # counts sub-dicts\n",
    "    bredth = len(next(iter(data[i].values()))) # counts dict bredth\n",
    "    print('Depth:',depth,'\\nBredth',bredth)\n",
    "    for key, value in data[i].items():\n",
    "        print('key',key)#, value) # Prints all keys and values\n",
    "    best.append(data[i]['best solution:']) # Degrees\n",
    "    \n",
    "    # Optional conversion to degrees (if underlying storage was radians)\n",
    "    if radians:\n",
    "        for j in np.arange(N_file[i]): # j for each device\n",
    "            best[i][6*j+3] = best[i][6*j+3]*180/np.pi\n",
    "            best[i][6*j+4] = best[i][6*j+4]*180/np.pi\n",
    "            best[i][6*j+5] = best[i][6*j+5]*180/np.pi\n",
    "    print(f\"File #{i+1}:\")\n",
    "    print(best[i].reshape(best[i].shape[0]//6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: 'best solution' and 'best solution fitness' are swapped in early anneal and gradient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[0]\n",
    "best_solution = data['best solution:']\n",
    "best_solution_fitness = data['best solution fitness']\n",
    "fitnesses = data['fitnesses']\n",
    "best_sols = data['best solution over generations']\n",
    "mean_fitnesses = data['mean fitnesses']\n",
    "std_fitnesses = data['std fitnesses']\n",
    "\n",
    "N = np.array(data['best solution:']).shape[0]//6\n",
    "num_generations = len(data['fitnesses']) # Adjusts to loaded data\n",
    "generations = list(np.arange(0,num_generations,1))\n",
    "\n",
    "for gen in generations:\n",
    "    best_sol = best_sols[gen].reshape((N, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph best fitness scores versus device count ###\n",
    "fitness = np.empty((len(N_file),))\n",
    "for i in range(len(N_file)): # i for each file\n",
    "    fitness[i] = data[i]['best solution fitness']\n",
    "\n",
    "plt.plot(N_file,fitness/1000.,label=\"Total IC\")\n",
    "for i,f in enumerate(fitness):\n",
    "    fitness[i] = f/N_file[i]\n",
    "plt.plot(N_file,fitness/1000.,label=\"IC per device\")\n",
    "#plt.yscale('log')\n",
    "plt.xticks(N_file)\n",
    "plt.ylabel('Information Capacity (kBits/s.)')\n",
    "plt.xlabel('Number of Devices')\n",
    "plt.title(f\"Fitness per device\\n({data[0]['type'][0]})\")\n",
    "plt.legend(loc='right')\n",
    "plt.savefig(path.join(out,f\"fitness_per_device-{data[0]['type'][0]}.pdf\"),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph the fitness gain over generations for all device counts ###\n",
    "if data[0]['type'][0] == 'anneal':\n",
    "    # Make graph relative to heat, not generations\n",
    "    fitness = []\n",
    "    heat = []\n",
    "    for i in np.arange(len(N_file)): # i for each file\n",
    "        f = []\n",
    "        h = []\n",
    "        for k in np.arange(len(data[i]['fitnesses'])): # k for each fitness/heat per file\n",
    "            f.append(data[i]['fitnesses'][k])\n",
    "            h.append(data[i]['temperature'][k])\n",
    "        fitness.append(f)\n",
    "        heat.append(h)\n",
    "\n",
    "    for i,n in enumerate(N_file):\n",
    "        if n == 1:\n",
    "            plt.plot(heat[i],fitness[i],label=f'{n} Device')\n",
    "        else:\n",
    "            plt.plot(heat[i],fitness[i],label=f'{n} Devices')\n",
    "    #plt.yscale('log')\n",
    "    plt.ylabel('Information Capacity (Bits/s.)')\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.xscale('log')\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.title(f\"Fitness per device over annealing temp.\\n({data[0]['type'][0]})\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(path.join(out,f\"fitness_over_generations-{data[0]['type'][0]}.pdf\"),dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    num_generations = len(data[0]['fitnesses'])\n",
    "    generations = list(np.arange(0,num_generations,1))\n",
    "    fitness = []# np.zeros((len(N_file),num_generations))\n",
    "    for i in np.arange(len(N_file)): # i for each file\n",
    "        fitness.append(data[i]['fitnesses'])\n",
    "\n",
    "    for i,n in enumerate(N_file):\n",
    "        if n == 1:\n",
    "            plt.plot(range(len(fitness[i])),np.array(fitness[i])/1000.,label=str(n)+' Device')\n",
    "        else:\n",
    "            plt.plot(range(len(fitness[i])),np.array(fitness[i])/1000.,label=str(n)+' Devices')\n",
    "    #plt.yscale('log')\n",
    "    plt.ylabel('Information Capacity (kBits/s.)')\n",
    "    plt.xlabel('Generations')\n",
    "    plt.title(f\"Fitness per device over generations\\n({data[0]['type'][0]})\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(path.join(out,f\"fitness_over_generations-{data[0]['type'][0]}.pdf\"),dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-generating voltages and performing SEPIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def virtual_macros(field,rings):\n",
    "    \"\"\"Aggregate a high-density DiSc lead field into virtual macro electrodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    field : np.ndarray\n",
    "        Lead field array of shape [X, Y, Z, 3, E] where E=64 for standard DiSc.\n",
    "    rings : int\n",
    "        Number of macro ring divisions (must be one of {1,2,4,8}). Determines how rows\n",
    "        are grouped vertically to form averaged macro contacts.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_field : np.ndarray\n",
    "        Reduced lead field with shape [X, Y, Z, 3, rings], each channel an averaged macro.\n",
    "        If input shape unsupported, original field returned with warning.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Averaging preserves overall magnitude by dividing by number of electrodes per macro.\n",
    "    - Assumes 8 columns x 8 rows layout (64 electrodes).\n",
    "    \"\"\"\n",
    "    rings = int(rings)\n",
    "    if field.shape[-1] == 64: # 64 ch DiSc\n",
    "        rows_per_ring = 8//rings\n",
    "        e_per_ring = rows_per_ring*8\n",
    "        new_field = np.zeros((field.shape[0],field.shape[1],field.shape[2],3,rings))\n",
    "        for r in range(rings): # r for each ring; top down\n",
    "            for c in range(8): # c for each column\n",
    "                for d in range(rows_per_ring): # d for column depth in each ring\n",
    "                    new_field[:,:,:,:,r] += field[:,:,:,:,c*8+r*rows_per_ring+d]\n",
    "        new_field /= e_per_ring\n",
    "        return new_field\n",
    "    else:\n",
    "        print(\"Incorrect input LF shape.\")\n",
    "        return field\n",
    "\n",
    "def field_reduce(field,count):\n",
    "    \"\"\"Reduce an original high-density lead field (e.g., 128 ch DiSc) to a lower sensor count.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    field : np.ndarray\n",
    "        Lead field array [X,Y,Z,3,E] with E typically 128.\n",
    "    count : int\n",
    "        Desired electrode count subset (currently supports 128 -> 64 reduction).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    field_out : np.ndarray | None\n",
    "        Reduced field with selected electrodes, or None if invalid count requested.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - For 128->64, keeps every other electrode index (even slicing).\n",
    "    - No interpolation performed; strict channel subsampling.\n",
    "    \"\"\"\n",
    "    if field.shape[-1] == 128:  # Starting with 128 channels\n",
    "        if count == 128:\n",
    "            field_out = field\n",
    "        if count == 64:\n",
    "            keep = np.arange(0,128,2)\n",
    "            field_out = field[:,:,:,:,keep]\n",
    "        else:\n",
    "            print(\"ERROR: No valid sensor count provided!\")\n",
    "            field_out = None # break process\n",
    "    else:\n",
    "        print(\"ERROR: Original LF not provided. LF unchanged.\")\n",
    "        field_out = field\n",
    "    return field_out\n",
    "\n",
    "def obtain_data(data, name):\n",
    "    \"\"\"Extract surface mesh components from a MATLAB .mat structure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        Result of `scipy.io.loadmat` containing region surface under key `name`.\n",
    "    name : str\n",
    "        Struct name containing (faces, vertices, normals) triple.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    header : bytes\n",
    "        MAT-file header metadata.\n",
    "    faces : np.ndarray (F,3)\n",
    "        Triangle indices.\n",
    "    vertices : np.ndarray (V,3)\n",
    "        Vertex coordinates.\n",
    "    normals : np.ndarray (V,3)\n",
    "        Per-vertex normal vectors.\n",
    "    \"\"\"\n",
    "    header = data['__header__']\n",
    "    faces = data[name][0][0][0]\n",
    "    vertices = data[name][0][0][1]\n",
    "    normals = data[name][0][0][2]\n",
    "\n",
    "    return header, faces, vertices, normals\n",
    "\n",
    "\n",
    "def recenter(vertices, reference):\n",
    "    \"\"\"Recenter a set of vertices around the center-of-mass of a reference vertex cloud.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vertices : np.ndarray (N,3)\n",
    "        Vertex coordinates to translate.\n",
    "    reference : np.ndarray (M,3)\n",
    "        Reference coordinates whose mean defines new origin.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vertices_modified : np.ndarray (N,3)\n",
    "        Recentered vertex array.\n",
    "    \"\"\"\n",
    "    center = np.mean(reference, axis = 0)\n",
    "    avg1 = center[0]\n",
    "    avg2 = center[1]\n",
    "    avg3 = center[2]\n",
    "    vertices_modified = vertices.copy()\n",
    "\n",
    "    # modify the data according to the midpoint\n",
    "    for idx in range(vertices.shape[0]):\n",
    "        vertices_modified[idx][0] -= avg1\n",
    "        vertices_modified[idx][1] -= avg2\n",
    "        vertices_modified[idx][2] -= avg3\n",
    "\n",
    "    return vertices_modified\n",
    "\n",
    "def get_rotmat(alpha, beta, gamma):\n",
    "    \"\"\"Compute combined rotation matrix from Euler angles (yaw, pitch, roll).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float\n",
    "        Yaw angle in radians (rotation about Z-axis).\n",
    "    beta : float\n",
    "        Pitch angle in radians (rotation about Y-axis).\n",
    "    gamma : float\n",
    "        Roll angle in radians (rotation about X-axis).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rot_mat : np.ndarray (3,3)\n",
    "        Rotation matrix R = yaw * pitch * roll.\n",
    "    \"\"\"\n",
    "    yaw = np.array([[np.cos(alpha),-np.sin(alpha),0],[np.sin(alpha),np.cos(alpha),0],[0,0,1]])\n",
    "    pitch = np.array([[np.cos(beta), 0, np.sin(beta)],[0,1,0],[-np.sin(beta),0,np.cos(beta)]])\n",
    "    roll = np.array([[1,0,0],[0,np.cos(gamma),-np.sin(gamma)],[0,np.sin(gamma),np.cos(gamma)]])\n",
    "    rot_mat = np.matmul(yaw, pitch)\n",
    "    rot_mat = np.matmul(rot_mat, roll)\n",
    "\n",
    "    return rot_mat\n",
    "\n",
    "def transform_vectorspace(vertices, normals, devpos, max_depth = None):\n",
    "    \"\"\"Transform cortical vertices/normals into device-local coordinate space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vertices : np.ndarray (N,3)\n",
    "        Recentered cortical layer vertices (mm).\n",
    "    normals : np.ndarray (N,3)\n",
    "        Corresponding normal vectors.\n",
    "    devpos : array-like (6,)\n",
    "        Device pose [x,y,z,alpha,beta,gamma] (mm, radians).\n",
    "    max_depth : float | None\n",
    "        Optional depth constraint (placeholder; not currently enforced).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dippos : np.ndarray (N,3)\n",
    "        Transformed dipole positions in voxel space (int-cast floats).\n",
    "    dipvec : np.ndarray (N,3)\n",
    "        Dipole vectors scaled by global `magnitude`.\n",
    "    rot_mat : np.ndarray (3,3)\n",
    "        Forward rotation matrix used (inverse applied during transformation).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Applies inverse rotation to align dipoles relative to device orientation.\n",
    "    - X/Y shifted by half field size to center within lead field volume.\n",
    "    - Unit conversion mm -> voxel index via global `scale`.\n",
    "    \"\"\"\n",
    "    rot_mat = get_rotmat(devpos[3],devpos[4],devpos[5])\n",
    "    inv_rot_mat = np.linalg.inv(rot_mat)\n",
    "    dippos = np.copy(vertices)\n",
    "    dipvec = np.copy(normals)\n",
    "\n",
    "    # translation\n",
    "    dippos-=devpos[:3]\n",
    "    dippos[0] += fields.shape[0]//2\n",
    "    dippos[1] += fields.shape[1]//2\n",
    "\n",
    "    # rotate vertices and normals based on the placement of the device\n",
    "    for idx in range(vertices.shape[0]):   # rotate for each vertex point and corresponding vectors\n",
    "        dippos[idx] = np.matmul(inv_rot_mat, dippos[idx])\n",
    "        dipvec[idx] = np.matmul(inv_rot_mat, dipvec[idx])\n",
    "\n",
    "    # Transfer position into leadfield space\n",
    "    dippos *= 1/scale\n",
    "\n",
    "    # to make it compatible with the lead field data\n",
    "    dippos = (dippos.astype('int')).astype('float')\n",
    "\n",
    "    # check if the depth restriction is exceeded\n",
    "    if max_depth is not None:\n",
    "        if dippos[2].any() < max_depth:\n",
    "            # Placeholder for depth enforcement logic\n",
    "            pass\n",
    "    \n",
    "    # Scale to dipole nAm magnitude, see value above\n",
    "    dipvec *= magnitude \n",
    "\n",
    "    return dippos, dipvec, rot_mat\n",
    "\n",
    "def trim_data(leadfield, vertices, normals):\n",
    "    \"\"\"Mask dipole positions lying outside the lead field bounds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    leadfield : np.ndarray\n",
    "        Lead field array [X,Y,Z,3,E].\n",
    "    vertices : np.ndarray (N,3)\n",
    "        Candidate dipole positions (voxel indices).\n",
    "    normals : np.ndarray (N,3)\n",
    "        Dipole moment vectors (scaled normals).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dippos : np.ndarray (N,3)\n",
    "        Positions with out-of-volume entries replaced by NaNs.\n",
    "    dipvec : np.ndarray (N,3)\n",
    "        Unchanged dipole vectors.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Z dimension assumed doubled relative to X/Y extents (checked explicitly).\n",
    "    - NaNs propagate through later metric calculations to exclude invalid points.\n",
    "    \"\"\"\n",
    "    dippos = np.copy(vertices)\n",
    "    dipvec = np.copy(normals)\n",
    "    # half of side length\n",
    "    len_half = leadfield.shape[0]//2\n",
    "\n",
    "    # um = 0\n",
    "    for idx in range(vertices.shape[0]):\n",
    "        if((np.abs(vertices[idx][0])>len_half) or (np.abs(vertices[idx][1])>len_half) or (vertices[idx][2])>len_half*2) or (vertices[idx][2]<0):\n",
    "            dippos[idx] = np.nan\n",
    "\n",
    "    return dippos, dipvec\n",
    "\n",
    "def calculate_voltage(fields, vertices, normals, vscale=10**6, montage = Montage, inter = False):\n",
    "    \"\"\"Compute voltage, SNR, and information capacity per dipole location.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fields : np.ndarray\n",
    "        Lead field array [X,Y,Z,3,E].\n",
    "    vertices : np.ndarray (N,3)\n",
    "        Transformed dipole positions (NaNs for invalid).\n",
    "    normals : np.ndarray (N,3)\n",
    "        Dipole vectors (nAm) oriented per device.\n",
    "    vscale : float\n",
    "        Scaling from Volts to microvolts (default 1e6).\n",
    "    montage : bool\n",
    "        If True use max-min montage spread where beneficial.\n",
    "    inter : bool\n",
    "        Placeholder for future inter-device montage support.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    opt_volt : np.ndarray (N,)\n",
    "        Optimal voltage per dipole (μV).\n",
    "    snr_list : np.ndarray (N,)\n",
    "        SNR values (dimensionless power ratio).\n",
    "    info_cap : np.ndarray (N,)\n",
    "        Information capacity estimates (bits/s.).\n",
    "    voltage : np.ndarray (N,E)\n",
    "        Raw electrode voltages before selection (μV after scaling).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - SNR = (Voltage / noise)^2; noise defined globally.\n",
    "    - Information Capacity = bandwidth * log2(1 + SNR).\n",
    "    - Optional ROI weighting applies multiplicative scaling if enabled.\n",
    "    \"\"\"\n",
    "    global roi_vertices_weights, do_weights\n",
    "    \n",
    "    # make copies of data\n",
    "    dippos = np.copy(vertices)\n",
    "    dipvec = np.copy(normals)\n",
    "\n",
    "    # create a list of field vectors applicable to the surface, [vertex index, electrode #]\n",
    "    voltage = np.empty((dippos.shape[0], fields.shape[-1]))\n",
    "    voltage[:]=np.nan\n",
    "    opt_volt = np.empty(dippos.shape[0])\n",
    "    opt_volt[:]=np.nan\n",
    "    \n",
    "    # calculate voltage for each vertex\n",
    "    for idx in range(dippos.shape[0]): # for each dipole\n",
    "        for e in range(fields.shape[-1]): # for every electrode\n",
    "            if not np.any(np.isnan(dippos[idx])): # if not nan, calculate and modify entry\n",
    "                try:\n",
    "                    lead_field = fields[int(dippos[idx][0]+fields.shape[0]//2), int(dippos[idx][1]+fields.shape[0]//2), int(dippos[idx][2]), :, e]\n",
    "                    voltage[idx, e] = np.dot(lead_field, dipvec[idx])\n",
    "                except: # error in finding lead field\n",
    "                    voltage[idx, e] = np.nan\n",
    "        if (not np.all(np.isnan(voltage[idx,:]))) and (not montage): \n",
    "            opt_volt[idx] = np.nanmax(np.abs(np.copy(voltage[idx]))) # get the optimal voltage across all electrodes\n",
    "        elif (not np.all(np.isnan(voltage[idx,:]))) and montage and (not inter):\n",
    "            single = np.nanmax(np.abs(np.copy(voltage[idx]))) # get the optimal voltage across all electrodes\n",
    "            mont = np.nanmax(voltage[idx]) - np.nanmin(voltage[idx])\n",
    "            if mont > single: # Use montage if it's better\n",
    "                opt_volt[idx] = mont\n",
    "            else: # Keep zero reference if it's better\n",
    "                opt_volt[idx] = single\n",
    "        # elif (not np.all(np.isnan(voltage[idx,:]))) and montage and inter:\n",
    "    voltage *= vscale # (sources, electrodes) for one device\n",
    "    opt_volt *= vscale #scale it to get the list of voltages\n",
    "    snr_list = np.square(np.copy(opt_volt)/noise) # get the list of snr values\n",
    "    info_cap = bandwidth*np.log2(1+snr_list) # get the list of information capacity\n",
    "    snr_list[np.isnan(snr_list)] = 0\n",
    "\n",
    "        # Scale with given weights for ROI subregions\n",
    "    if do_weights:\n",
    "        opt_volt = np.multiply(opt_volt,roi_vertices_weights)\n",
    "        snr_list = np.multiply(snr_list,roi_vertices_weights)\n",
    "        info_cap = np.multiply(info_cap,roi_vertices_weights)\n",
    "\n",
    "\n",
    "    return (opt_volt, snr_list, info_cap, voltage)\n",
    "\n",
    "def find_snr(devpos,value):\n",
    "    \"\"\"Aggregate sensing metrics across multiple devices for a given pose list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    devpos : np.ndarray (N*6,)\n",
    "        Flat array of concatenated device poses [x,y,z,alpha,beta,gamma] * N devices.\n",
    "    value : int\n",
    "        Metric selector: 0 voltage(montaged), 1 SNR, 2 IC, 3 or -1 raw voltage tensor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_dev_volt : np.ndarray\n",
    "        Metric array with shape depending on selection. For IC: (sources, N). For raw voltage:\n",
    "        (sources, N, electrodes). For others: (sources, N, electrodes) pre-aggregation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses global `fields`, `roi_normals`, `recentered_roi`, `Montage`.\n",
    "    - Result may contain NaNs which are later converted or aggregated.\n",
    "    \"\"\"\n",
    "    devpos = devpos.reshape((N, 6))\n",
    "    if value == 2: # IC\n",
    "        all_dev_volt = np.empty((recentered_roi.shape[0], N))\n",
    "    else:\n",
    "        all_dev_volt = np.empty((recentered_roi.shape[0], N, fields.shape[-1]))\n",
    "    for idx in range(len(devpos)):\n",
    "        dippos, dipvec, rotmat = transform_vectorspace(recentered_roi, roi_normals, devpos[idx])\n",
    "        dippos_adj, dipvec_adj = trim_data(fields, dippos, dipvec)\n",
    "        # Index decides volt, snr, IC, or voltage by(source,electrodes)\n",
    "        if value == 2: # IC\n",
    "            all_dev_volt[:, idx] = calculate_voltage(fields, dippos_adj, dipvec_adj, montage = Montage, inter = False)[value]\n",
    "        else:\n",
    "            all_dev_volt[:, idx,:] = calculate_voltage(fields, dippos_adj, dipvec_adj, montage = Montage, inter = False)[value]\n",
    "    \n",
    "    voltage = np.nanmax(np.nan_to_num(all_dev_volt, nan=0), axis=1)\n",
    "    total = np.nansum(voltage)\n",
    "    # print(\"total IC:\", total, \"Bits/s\")\n",
    "    return all_dev_volt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voltage calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjust fields to desired size ###\n",
    "disc_sensors = 64 # 64 or 128 for DiSc; sensors reduced evenly  in grid\n",
    "fields = field_reduce(fields,disc_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate virtual macros from DiSc ###\n",
    "if macros:\n",
    "    fields = virtual_macros(fields,virtual_rings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load brain and ROI ###\n",
    "# Values to collect\n",
    "brain_vals = []\n",
    "brain_vallen = [[],[],[]] # summative index for each start point\n",
    "brain_count = [0,0,0] # running index sum\n",
    "roi_vals = []\n",
    "roi_vallen = [[],[],[]]\n",
    "roi_count = [0,0,0]\n",
    "# Start indexing counts per file\n",
    "#for b in brain_data:\n",
    "#    brain_count.append(0)\n",
    "#for b in roi_data:\n",
    "#    roi_count.append(0)\n",
    "\n",
    "# Load each file\n",
    "for i,fb in enumerate(brain_data):\n",
    "    _, brain_faces, brain_vertices, brain_normals = obtain_data(brain_data[i], brain_label[i]) # 'ans' or 'brain'\n",
    "    brain_vals.append([brain_faces,brain_vertices,brain_normals])\n",
    "    for k in range(3):\n",
    "        brain_vallen[k].append(brain_vals[i][k].shape[0] + brain_count[k])\n",
    "        brain_count[k] += brain_vals[i][k].shape[0]\n",
    "for i,fb in enumerate(roi_data):\n",
    "    _, roi_faces, roi_vertices, roi_normals = obtain_data(roi_data[i], roi_label[i]) # 'ans' or 'auditory roi'\n",
    "    roi_vals.append([roi_faces,roi_vertices,roi_normals])\n",
    "    for k in range(3):\n",
    "        roi_vallen[k].append(roi_vals[i][k].shape[0] + roi_count[k])\n",
    "        roi_count[k] += roi_vals[i][k].shape[0]\n",
    "\n",
    "# Initialize final variables\n",
    "brain_faces = np.empty((brain_vallen[0][-1],3))\n",
    "brain_vertices = np.empty((brain_vallen[1][-1],3))\n",
    "brain_normals = np.empty((brain_vallen[2][-1],3))\n",
    "roi_faces = np.empty((roi_vallen[0][-1],3))\n",
    "roi_vertices = np.empty((roi_vallen[1][-1],3))\n",
    "roi_normals = np.empty((roi_vallen[2][-1],3))\n",
    "roi_vertices_weights = np.zeros((roi_vallen[1][-1],)) # matching index weights\n",
    "\n",
    "# Merge file variables\n",
    "for i in range(len(brain_data)): # i for each file index\n",
    "    if i == 0: # first file starts from zero in lists\n",
    "        brain_faces[0:brain_vallen[0][i]] = brain_vals[i][0]\n",
    "        brain_vertices[0:brain_vallen[1][i]] = brain_vals[i][1]\n",
    "        brain_normals[0:brain_vallen[2][i]] = brain_vals[i][2]\n",
    "    else:\n",
    "        brain_faces[brain_vallen[0][i-1]:brain_vallen[0][i]] = brain_vals[i][0]\n",
    "        brain_vertices[brain_vallen[1][i-1]:brain_vallen[1][i]] = brain_vals[i][1]\n",
    "        brain_normals[brain_vallen[2][i-1]:brain_vallen[2][i]] = brain_vals[i][2]\n",
    "for i in range(len(roi_data)): # i for each roi file\n",
    "    if i == 0: # first file starts from zero in lists\n",
    "        roi_faces[0:roi_vallen[0][i]] = roi_vals[i][0]\n",
    "        roi_vertices[0:roi_vallen[1][i]] = roi_vals[i][1]\n",
    "        roi_normals[0:roi_vallen[2][i]] = roi_vals[i][2]\n",
    "\n",
    "        roi_vertices_weights[0:roi_vallen[1][i]] = np.repeat(roi_weights[i],roi_vallen[1][i])\n",
    "    else:\n",
    "        roi_faces[roi_vallen[0][i-1]:roi_vallen[0][i]] = roi_vals[i][0]\n",
    "        roi_vertices[roi_vallen[1][i-1]:roi_vallen[1][i]] = roi_vals[i][1]\n",
    "        roi_normals[roi_vallen[2][i-1]:roi_vallen[2][i]] = roi_vals[i][2]\n",
    "        roi_vertices_weights[roi_vallen[1][i-1]:roi_vallen[1][i]] += np.repeat(roi_weights[i],(roi_vallen[1][i] - roi_vallen[1][i-1]))\n",
    "\n",
    "# Recenter total ROI based on total brain\n",
    "recentered_roi = recenter(roi_vertices, brain_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate voltages for best position set ###\n",
    "voltages = []  # Must be flexible list for different sensor counts\n",
    "infocap = [] # Collect IC values\n",
    "sensor_count = fields.shape[-1]\n",
    "\n",
    "# Manual trajectory injection\n",
    "manual = False\n",
    "\n",
    "# Manual trajectory demo\n",
    "manual_trajectories = [ # Each inside list is (N*6,) for coordinates for N devices\n",
    "    np.array([0, 0, 5, 0, 0, 180]),\n",
    "    np.array([0, 3, 4.5, 0, 0, 180,\n",
    "              0, -3, 6.5, 0, 0, 180]),\n",
    "    np.array([-.5, -3, 6, 0, 0, 180,\n",
    "              -.5, 3, 4.5, 0, 0, 180,\n",
    "              2, 0, 5, 0, 0, 180])\n",
    "]\n",
    "\"\"\" \n",
    "# ACC/ DL-PFC; R/L inner cortex\n",
    "manual_trajectories = [ # Each inside list is (N*6,) for coordinates for N devices\n",
    "    np.array([-3, 25, 18, 0, 0, 180]),\n",
    "    np.array([-5, 27, 18, 0, 0, 180,\n",
    "             5, 27, 18, 0, 0, 180]),\n",
    "    np.array([-10, 28, 18, 0, 0, 180,\n",
    "             7, 28, 18, 0, 0, 180,\n",
    "             -3, 23, 18, 0, 0, 180]),\n",
    "    np.array([-5, 24, 18, 0, 0, 180,\n",
    "              5, 24, 18, 0, 0, 180,\n",
    "              -13, 27, 18, 0, 0, 180,\n",
    "              10, 27, 18, 0, 0, 180]),\n",
    "    np.array([-8, 28, 18, 0, 0, 180,\n",
    "              5, 28, 18, 0, 0, 180,\n",
    "              -3, 23, 18, 0, 0, 180,\n",
    "              -16, 24, 19, 0, 0, 180,\n",
    "              13, 24, 19, 0, 0, 180])\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "if manual: # Rebuild device count list\n",
    "    N_file = []\n",
    "    for t in manual_trajectories:\n",
    "        N_file.append(t.shape[0]//6)\n",
    "        if t.shape[0]%6 != 0:\n",
    "            print(\"Error: Manual trajectory incorrect.\")\n",
    "            break\n",
    "\n",
    "# Main loop\n",
    "for i,N in enumerate(N_file): # each i file of N devices\n",
    "    if manual:\n",
    "        voltages.append(find_snr(manual_trajectories[i],-1))\n",
    "        infocap.append(find_snr(manual_trajectories[i],2))\n",
    "    else:\n",
    "        voltages.append(find_snr(best[i],-1))\n",
    "        infocap.append(find_snr(best[i],2))\n",
    "    voltages[i] = voltages[i].reshape(-1,N*sensor_count)\n",
    "    print(f\"Done with {N} devices with voltage array shape: {voltages[i].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess voltage and IC\n",
    "Not used in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(infocap[0].shape)\n",
    "print(np.count_nonzero(np.isnan(infocap[0])))\n",
    "print(np.count_nonzero(np.isnan(np.sum(voltages[0],axis=1))))\n",
    "print(np.where(~np.isnan(infocap[0]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assess voltages ###\n",
    "print(\"Voltage summary\")\n",
    "for i in range(len(N_file)):\n",
    "    print(\"N:\",N_file[i])\n",
    "    print(\"Mean:\",np.nanmean(voltages[i]))\n",
    "    #print(\"Max:\",np.nanmax(voltages[i]))\n",
    "    #print(\"Min:\",np.nanmin(voltages[i]))\n",
    "    print(\"STD:\",np.nanstd(voltages[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assess Information Capacity ###\n",
    "IC_threshold = 160 # Bits/s (X = 100log2(SNR+1))\n",
    "print(\"Information capacity summary\")\n",
    "for i in range(len(N_file)):\n",
    "    print(\"N:\",N_file[i])\n",
    "    print(\"Mean:\",np.nanmean(np.nanmax(np.nan_to_num(infocap[i]),axis=1)))\n",
    "    #print(\"Max:\",np.nanmax(np.nanmax(infocap[i],axis=1)))\n",
    "    #print(\"Min:\",np.nanmin(np.nanmax(infocap[i],axis=1)))\n",
    "    print(\"STD:\",np.nanstd(np.nanmax(np.nan_to_num(infocap[i]),axis=1)))\n",
    "    print(\"Total IC:\",np.nansum(np.nanmax(np.nan_to_num(infocap[i]),axis=1)),\"Bits/s.\")\n",
    "    print(\"Percent of sources above threshold:\",np.count_nonzero(np.nanmax(np.nan_to_num(infocap[i]),axis=1) > IC_threshold)/infocap[i].shape[0]*100,\n",
    "          \"%\",f\" for a threshold of {IC_threshold} Bits/s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot IC distribution for one device by distance of sources ###\n",
    "# Avoid NaN indices that are not observed by the dataset\n",
    "notnan = np.where(~np.isnan(infocap[0]))[0] # safe indices\n",
    "\n",
    "# Distance to each point from the device assigned position; matched indices to voltage\n",
    "distance = recentered_roi - best[0][0:3]\n",
    "distance = np.sqrt(np.sum(distance**2,axis=-1))[notnan]\n",
    "\n",
    "# Plot source distances\n",
    "plt.hist(distance,bins=50, color='lightblue',edgecolor='black')\n",
    "plt.xlim([0,30])\n",
    "plt.xlabel('Device-Source Distance (mm)')\n",
    "plt.ylabel('Source Count')\n",
    "plt.savefig(path.join(out,f\"8_Distance-distribution.png\"),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot IC voilin for DiSc and vSEEG at varying range\n",
    "bins = [0,7.5,15,22.5,30] # mm distance groups\n",
    "binned_ic = [[] for _ in range(len(bins)-1)]\n",
    "positions = []\n",
    "for i in (range(len(bins)-1)):\n",
    "    positions.append((bins[i]+bins[i+1])/2)\n",
    "positions = np.asarray(positions)\n",
    "\n",
    "for dist, ic in zip(distance,infocap[0][notnan]):\n",
    "    bin_idx = np.digitize(dist,bins) - 1\n",
    "    if 0<=bin_idx<(len(bins)-1):\n",
    "        binned_ic[bin_idx].append(ic[0])\n",
    "\n",
    "plt.violinplot(binned_ic,positions=positions,showmeans=True,widths=(bins[1]-bins[0])/1.5)\n",
    "plt.xticks(bins)\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlabel('Binned Distance (mm)')\n",
    "plt.ylabel('Info. Cap. Distribution')\n",
    "plt.savefig(path.join(out,f\"8_IC-distribution-{voltages[0].shape[-1]}e.png\"),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEPIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEPIO train/test with variable replicates ###\n",
    "MC_cycles = 1 # How many Monte-Carlo cycles to complete and average\n",
    "rep_per_max = 1 # (INT) replicates per max sensors; 1 for DiSc; 4 for SEEG; 8 for ECoG\n",
    "sensor_div = 16 # Sensor testing granularity (defines sensor_range steps)\n",
    "noise_mult = 10. # Boost noise uniformly; 5-10 for increased classification difficulty\n",
    "totalcoefs = [] # Coefficients per test set\n",
    "totalacc = [] # Test acc (sensor space)\n",
    "spatialacc = [] # Test acc (source space)\n",
    "sensor_list = [] # list of sensor_range\n",
    "\n",
    "for i, N in enumerate(N_file):\n",
    "    print(f\"Starting loop {i+1} with {N} devices.\")\n",
    "    MCcoefs, MCaccs, MCSaccs, sensor_range = sepio.mc_train(X=voltages[i],y=None,Xt=None,yt=None,sensor_div=sensor_div,\n",
    "    MCcount=MC_cycles,noise=noise*noise_mult,replicates=rep_per_max*voltages[i].shape[-1],nbasis=20,spatial=False,l1=0.001,rep_subdiv=8)\n",
    "    totalcoefs.append(MCcoefs)\n",
    "    totalacc.append(MCaccs)\n",
    "    spatialacc.append(MCSaccs)\n",
    "    sensor_list.append(sensor_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check top-N sensors alone\n",
    "if False:\n",
    "    topN = 16\n",
    "    dataset = 0 # index for file\n",
    "    rep_per_max = 1\n",
    "    sensor_div = 2 # Sensor testing granularity (defines sensor_range steps)\n",
    "    sensors = np.flip(np.argsort(np.mean(np.abs(totalcoefs[dataset]),axis=1)))[:16]\n",
    "    TNcoefs, TNaccs, TNSaccs, TNsensor_range = sepio.mc_train(X=voltages[dataset][:,sensors],y=None,Xt=None,yt=None,sensor_div=sensor_div,\n",
    "    MCcount=1,noise=4.1,replicates=rep_per_max*voltages[dataset][:,sensors].shape[-1],nbasis=20,spatial=False,l1=0.001,rep_subdiv=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting accuracies for each device set ###\n",
    "# Produces line plots of SEPIO classification accuracy (%) versus sensor count.\n",
    "# Logic:\n",
    "# - totalacc[i] holds accuracy progression for device configuration i across tested sensor subsets.\n",
    "# - For multi-device cases, sensor axis is total sensors (device_count * sensors_per_device).\n",
    "# - Sensor counts derived from range(0, sensor_count*N, sensor_div) shifted by +sensor_div to start at first non-zero group.\n",
    "# - Y-axis scaled to percent; X-axis increments enforce granularity defined by sensor_div.\n",
    "for i,N in enumerate(N_file):\n",
    "    if i < len(totalacc):\n",
    "        if N == 1:\n",
    "            plt.plot(np.arange(0,sensor_count,sensor_div)+sensor_div,100*totalacc[i],label=\"1 Device\")\n",
    "        else:\n",
    "            plt.plot(np.arange(0,sensor_count*N,sensor_div)+sensor_div,100*totalacc[i],label=f\"{N} Devices\")\n",
    "\n",
    "plt.ylabel(f\"% Accuracy for {voltages[0].shape[0]} classes\")\n",
    "plt.xlabel(\"Sensor count\")\n",
    "plt.title(f\"SEPIO classification accuracy\\n({data[0]['type'][0]})\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.savefig(path.join(out,f\"8_SEPIO_total-{data[0]['type'][0]}.png\"),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot sensorXdevice accuracy contour ###\n",
    "# Builds a triangulated contour over (sensor_count, device_count) -> accuracy space.\n",
    "# Data preparation:\n",
    "# - xs: sensor counts (multiples of sensor_div)\n",
    "# - ys: device counts (from N_file)\n",
    "# - zs: accuracy values (percent) from totalacc arrays\n",
    "# Visualization:\n",
    "# - `plot_divs` controls contour granularity.\n",
    "# - Colorbar labels accuracy distribution; can reveal diminishing returns when scaling devices vs sensors.\n",
    "\n",
    "# Assign plot values\n",
    "xs = []\n",
    "ys = []\n",
    "zs = []\n",
    "for n in range(len(totalacc)): # k is acc\n",
    "    # n+1 for each device count\n",
    "    x = []\n",
    "    y = []\n",
    "    for s in range(totalacc[n].size):\n",
    "        # (s+1)*16 for each sensor count\n",
    "        xs.append((s+1)*sensor_div)\n",
    "        ys.append(N_file[n])\n",
    "        zs.append(totalacc[n][s]*100)\n",
    "\n",
    "# Plot settings\n",
    "plot_divs = 18 # number of plot divisions\n",
    "#plt.tricontour(xs,ys,zs,levels=plot_divs,colors='k') # Adds lines between regions\n",
    "pt = plt.tricontourf(xs,ys,zs,levels=plot_divs,cmap=\"RdBu_r\") # colors in regions\n",
    "\n",
    "plt.yticks(N_file[:len(totalacc)])\n",
    "plt.xlim([0,max(N_file[:len(totalacc)])*fields.shape[-1]])\n",
    "plt.ylabel(\"Device count\")\n",
    "plt.xlabel(\"Sensor count\")\n",
    "plt.title(f\"SEPIO by device and sensor count\\n({data[0]['type'][0]})\")\n",
    "cb = plt.colorbar(pt)\n",
    "cb.set_label(\"% Accuracy\")\n",
    "plt.savefig(path.join(out,f\"8_SEPIO_SxN-{data[0]['type'][0]}.png\"),dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
