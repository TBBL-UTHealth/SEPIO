{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess MDPO Results\n",
    "Loads MDPO pickle file and evaluates training over time.\n",
    "Also equipped to perform SEPIO on device placements to determine the limits of classificaiton accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports and Files ###\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import path\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.insert(0, path.join('../..'))\n",
    "from modules.leadfield_importer import FieldImporter\n",
    "from scipy.io import loadmat\n",
    "from modules.SEPIO import sepio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides graphing of MDPO results and performs SEPIO on desired sensor sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding device locations\n",
    "Settings __MUST__ match those chosen prior in optimization!\n",
    "\n",
    "Can potentially switch devices __IF__ they are the same shape lead fields (excluding sensor axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brainsuite files\n",
    "# List all components of the 'full brain' and 'ROI', where ROI is weighted for each\n",
    "folder = r\"...\\SEPIO_dataset\" # Point to dataset folder\n",
    "\n",
    "# Auditory cortex test\n",
    "brain_data = [loadmat(path.join(folder,'MDPO_data','brain.mat'))]\n",
    "roi_data = [loadmat(path.join(folder,'MDPO_data','auditory_roi.mat'))]\n",
    "roi_weights = [1]\n",
    "\n",
    "do_weights = False\n",
    "\n",
    "# Define dipoles\n",
    "# Magnitude is 0.5e-9 (nAm) for real signals or 20e-9 (nAm) for phantom simulations\n",
    "magnitude = 0.5e-9 # nAm; Default power at given cortical column length\n",
    "dipole_base_length = 2.5 # mm; Default cortical column length\n",
    "\n",
    "# Load lead fields\n",
    "fields_files = path.join(folder,'leadfields',\"SEEG_8e_500um_1500pitch.npz\")\n",
    "#fields_files = path.join(folder,'leadfields',\"DISC_30mm_p2-5Sm_MacChr.npz\")\n",
    "#fields_files = path.join(folder,'leadfields',\"ECoG_1mm_500umgrid.npz\")\n",
    "\n",
    "# DiSc specific parameters\n",
    "macros = False # Bool; Reduce DiSc to virtual macros\n",
    "virtual_rings = 4 # Must be one of [1,2,4,8]\n",
    "\n",
    "# Processing parameters\n",
    "sensor_div = 8 # Sensor step size for SEPIO; MUST SATISFY -> sensor_max % sensor_div = 0\n",
    "                # 16 for DiSc; 2/4 for SEEG (8, 16, 18 sensors)\n",
    "scale = 0.5 # mm; voxel size\n",
    "cl_wd = 1.0 # mm diameter to clear; device may be slightly offset\n",
    "noise = 4.1 # uV RMS noise; 4.1 for DiSc; 2.7 for SEEG; Varing for ECoG based on diameter\n",
    "bandwidth = 100 # Samples/second; Recording system limit or reasonable maximum\n",
    "Montage = False # Montage devices\n",
    "field_importer = FieldImporter()\n",
    "field = field_importer.load(fields_files)\n",
    "num_electrodes = np.shape(field_importer.fields)[4]\n",
    "fields = field_importer.fields\n",
    "midpoint = fields.shape[0]//2  # field midpoint index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GA file locations\n",
    "out = path.join(folder,\"outputs\")\n",
    "\n",
    "# Auditory cortex used for paper\n",
    "files = [r\"20241024_1054-genetic-1.pkl\",r\"20241024_1115-genetic-2.pkl\",r\"20241024_1146-genetic-3.pkl\",\n",
    "         r\"20241024_1227-genetic-4.pkl\",r\"20241024_1313-genetic-5.pkl\"]\n",
    "\n",
    "# Assign file N\n",
    "N_file = np.arange(1,len(files)+1) # Number of devices used for each file; INT only; Could be made automatic from file contents\n",
    "pickle_files = []\n",
    "for f in files:\n",
    "    pickle_files.append(path.join(folder,'MDPO_data',f))\n",
    "\n",
    "# Load Pickle file\n",
    "data = []\n",
    "for p in pickle_files:\n",
    "    with open(p,'rb') as f:\n",
    "        data.append(pickle.load(f))\n",
    "\n",
    "# Assess data dict\n",
    "''' KEYS\n",
    "'type'\n",
    "'best solution:'\n",
    "'best solution fitness'\n",
    "'fitnesses'\n",
    "'solutions'\n",
    "'device names'\n",
    "'device types'\n",
    "'field files'\n",
    "'N per device'\n",
    "'LF scale'\n",
    "'dev diameter'\n",
    "'dev depth'\n",
    "'dev bacend'\n",
    "'voltage scale'\n",
    "'dev bandwidth'\n",
    "'depth limits'\n",
    "'angle limits'\n",
    "'montaged'\n",
    "''' \n",
    "\n",
    "best = []\n",
    "radians = False # Return radians or degrees; MUST be FALSE for SEPIO; MUST be TRUE for visualization plotting\n",
    "if radians:\n",
    "    print(\"!!!DISABLE `radians` to use SEPIO!!!\")\n",
    "print('Data')\n",
    "for i,f in enumerate(files):\n",
    "    print(\"N =\",N_file[i])\n",
    "    depth = sum(isinstance(v, dict) for v in data[i].values()) # counts sub-dicts\n",
    "    bredth = len(next(iter(data[i].values()))) # counts dict bredth\n",
    "    print('Depth:',depth,'\\nBredth',bredth)\n",
    "    for key, value in data[i].items():\n",
    "        print('key',key)#, value) # Prints all keys and values\n",
    "    best.append(data[i]['best solution:']) # Degrees\n",
    "    \n",
    "    # Convert to degrees\n",
    "    if radians:\n",
    "        for j in np.arange(N_file[i]): # j for each device\n",
    "            best[i][6*j+3] = best[i][6*j+3]*180/np.pi\n",
    "            best[i][6*j+4] = best[i][6*j+4]*180/np.pi\n",
    "            best[i][6*j+5] = best[i][6*j+5]*180/np.pi\n",
    "    print(f\"File #{i+1}:\")\n",
    "    print(best[i].reshape(best[i].shape[0]//6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: 'best solution' and 'best solution fitness' are swapped in early anneal and gradient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[0]\n",
    "best_solution = data['best solution:']\n",
    "best_solution_fitness = data['best solution fitness']\n",
    "fitnesses = data['fitnesses']\n",
    "best_sols = data['best solution over generations']\n",
    "mean_fitnesses = data['mean fitnesses']\n",
    "std_fitnesses = data['std fitnesses']\n",
    "\n",
    "N = np.array(data['best solution:']).shape[0]//6\n",
    "num_generations = len(data['fitnesses']) # Adjusts to loaded data\n",
    "generations = list(np.arange(0,num_generations,1))\n",
    "\n",
    "for gen in generations:\n",
    "    best_sol = best_sols[gen].reshape((N, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph best fitness scores versus device count ###\n",
    "fitness = np.empty((len(N_file),))\n",
    "for i in range(len(N_file)): # i for each file\n",
    "    fitness[i] = data[i]['best solution fitness']\n",
    "\n",
    "plt.plot(N_file,fitness,label=\"Total IC\")\n",
    "for i,f in enumerate(fitness):\n",
    "    fitness[i] = f/N_file[i]\n",
    "plt.plot(N_file,fitness,label=\"IC per device\")\n",
    "#plt.yscale('log')\n",
    "plt.xticks(N_file)\n",
    "plt.ylabel('Information Capacity (Bits/s.)')\n",
    "plt.xlabel('Number of Devices')\n",
    "plt.title(f\"Fitness per device\\n({data[0]['type'][0]})\")\n",
    "plt.legend(loc='right')\n",
    "plt.savefig(path.join(out,f\"8_best_fitness-{data[0]['type'][0]}.png\"),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph the fitness gain over generations for all device counts ###\n",
    "if data[0]['type'][0] == 'anneal':\n",
    "    # Make graph relative to heat, not generations\n",
    "    fitness = []\n",
    "    heat = []\n",
    "    for i in np.arange(len(N_file)): # i for each file\n",
    "        f = []\n",
    "        h = []\n",
    "        for k in np.arange(len(data[i]['fitnesses'])): # k for each fitness/heat per file\n",
    "            f.append(data[i]['fitnesses'][k])\n",
    "            h.append(data[i]['temperature'][k])\n",
    "        fitness.append(f)\n",
    "        heat.append(h)\n",
    "\n",
    "    for i,n in enumerate(N_file):\n",
    "        if n == 1:\n",
    "            plt.plot(heat[i],fitness[i],label=f'{n} Device')\n",
    "        else:\n",
    "            plt.plot(heat[i],fitness[i],label=f'{n} Devices')\n",
    "    #plt.yscale('log')\n",
    "    plt.ylabel('Information Capacity (Bits/s.)')\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.xscale('log')\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.title(f\"Fitness per device over annealing temp.\\n({data[0]['type'][0]})\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(path.join(out,f\"8_fitness_over_generations-{data[0]['type'][0]}.png\"),dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    num_generations = len(data[0]['fitnesses'])\n",
    "    generations = list(np.arange(0,num_generations,1))\n",
    "    fitness = np.zeros((len(N_file),num_generations))\n",
    "    for i in np.arange(len(N_file)): # i for each file\n",
    "        fitness[i] += data[i]['fitnesses']\n",
    "\n",
    "    for i,n in enumerate(N_file):\n",
    "        if n == 1:\n",
    "            plt.plot(generations,fitness[i],label=str(n)+' Device')\n",
    "        else:\n",
    "            plt.plot(generations,fitness[i],label=str(n)+' Devices')\n",
    "    #plt.yscale('log')\n",
    "    plt.ylabel('Information Capacity (Bits/s.)')\n",
    "    plt.xlabel('Generations')\n",
    "    plt.title(f\"Fitness per device over generations\\n({data[0]['type'][0]})\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(path.join(out,f\"8_fitness_over_generations-{data[0]['type'][0]}.png\"),dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-generating voltages and performing SEPIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def virtual_macros(field,rings):\n",
    "    # Reduce a DiSc LF into virtual macros\n",
    "    # 'rings' defines how many macros to divide into (1,2,4,or 8)\n",
    "    rings = int(rings)\n",
    "    if field.shape[-1] == 64: # 64 ch DiSc\n",
    "        rows_per_ring = 8//rings\n",
    "        e_per_ring = rows_per_ring*8\n",
    "        new_field = np.zeros((field.shape[0],field.shape[1],field.shape[2],3,rings))\n",
    "        for r in range(rings): # r for each ring; top down\n",
    "            for c in range(8): # c for each column\n",
    "                for d in range(rows_per_ring): # d for column depth in each ring\n",
    "                    new_field[:,:,:,:,r] += field[:,:,:,:,c*8+r*rows_per_ring+d]\n",
    "        new_field /= e_per_ring\n",
    "        return new_field\n",
    "    else:\n",
    "        print(\"Incorrect input LF shape.\")\n",
    "        return field\n",
    "\n",
    "def field_reduce(field,count):\n",
    "    if field.shape[-1] == 128: # Starting with 128 channels\n",
    "        if count == 128:\n",
    "            field_out = field\n",
    "        if count == 64:\n",
    "            keep = np.arange(0,128,2)\n",
    "            field_out = field[:,:,:,:,keep]\n",
    "        else:\n",
    "            print(\"ERROR: No valid sensor count provided!\")\n",
    "            field_out = None # break process\n",
    "    else:\n",
    "        print(\"ERROR: Original LF not provided. LF unchanged.\")\n",
    "        field_out = field\n",
    "    return field_out\n",
    "\n",
    "def obtain_data(data, name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract data from a given file\n",
    "    \n",
    "    inputs: \n",
    "        - data = data obtained by the function \"loadmat\".\n",
    "        - name = a string that is the name of that brain region in the filename saved in the user's computer\n",
    "\n",
    "    output: header, faces, vertices, and normals from the provided file\n",
    "    \"\"\"\n",
    "\n",
    "    # creating corresponding lists in python in place of matlab variables\n",
    "    header = data['__header__']\n",
    "    faces = data[name][0][0][0]\n",
    "    vertices = data[name][0][0][1]\n",
    "    normals = data[name][0][0][2]\n",
    "\n",
    "    return header, faces, vertices, normals\n",
    "\n",
    "\n",
    "def recenter(vertices, reference):\n",
    "\n",
    "    \"\"\"\n",
    "    Recenter the data\n",
    "    \n",
    "    inputs: \n",
    "        - vertices: vertices extracted from a specific brain region\n",
    "        - reference for avg1, avg2, avg3 calculated from the reference brian region\n",
    "        \n",
    "    outputs: vertices_modified, the data in vertices, recentered based on avg1, avg2, and avg3\n",
    "    \"\"\"\n",
    "\n",
    "    # find midpoint\n",
    "    center = np.mean(reference, axis = 0)\n",
    "    avg1 = center[0]\n",
    "    avg2 = center[1]\n",
    "    avg3 = center[2]\n",
    "    # print(avg1, avg2, avg3)\n",
    "    \n",
    "    # create a copy of the vertices\n",
    "    vertices_modified = vertices.copy()\n",
    "\n",
    "    # modify the data according to the midpoint\n",
    "    for idx in range(vertices.shape[0]):\n",
    "        vertices_modified[idx][0] -= avg1\n",
    "        vertices_modified[idx][1] -= avg2\n",
    "        vertices_modified[idx][2] -= avg3\n",
    "\n",
    "    return vertices_modified\n",
    "\n",
    "def get_rotmat(alpha, beta, gamma):\n",
    "    \"\"\"\n",
    "    Given device position in standard form, return rotation matrix\n",
    "    \"\"\"\n",
    "    # define rotation matrix \n",
    "    yaw = np.array([[np.cos(alpha),-np.sin(alpha),0],[np.sin(alpha),np.cos(alpha),0],[0,0,1]])\n",
    "    pitch = np.array([[np.cos(beta), 0, np.sin(beta)],[0,1,0],[-np.sin(beta),0,np.cos(beta)]])\n",
    "    roll = np.array([[1,0,0],[0,np.cos(gamma),-np.sin(gamma)],[0,np.sin(gamma),np.cos(gamma)]])\n",
    "    rot_mat = np.matmul(yaw, pitch)\n",
    "    rot_mat = np.matmul(rot_mat, roll)\n",
    "\n",
    "    return rot_mat\n",
    "\n",
    "def transform_vectorspace(vertices, normals, devpos, max_depth = None):\n",
    "    \"\"\"\n",
    "    Transforms the MRI space depending on the device location in order to calculate voltage\n",
    "    inputs:\n",
    "        - vertices, array of vertices wrs to the original MRI coordinates\n",
    "        - normals, array of normal vectors wrs to the original MRI coordinates\n",
    "        - devpos, manually defined device position\n",
    "    output: \n",
    "        - dippos, shifted and rotated vertices depending on the device location\n",
    "        - dipvec, shifted and rotated normal vectors depending on the device location\n",
    "    \"\"\"\n",
    "\n",
    "    rot_mat = get_rotmat(devpos[3],devpos[4],devpos[5])\n",
    "    inv_rot_mat = np.linalg.inv(rot_mat)\n",
    "\n",
    "    ###\n",
    "    ##### No transformation done above this line #####\n",
    "    ###\n",
    "\n",
    "    dippos = np.copy(vertices)\n",
    "    dipvec = np.copy(normals)\n",
    "\n",
    "    # translation\n",
    "    dippos-=devpos[:3]\n",
    "    dippos[0] += fields.shape[0]//2\n",
    "    dippos[1] += fields.shape[1]//2\n",
    "\n",
    "    # rotate vertices and normals based on the placement of the device\n",
    "    for idx in range(vertices.shape[0]):   # rotate for each vertex point and corresponding vectors\n",
    "        dippos[idx] = np.matmul(inv_rot_mat, dippos[idx])\n",
    "        dipvec[idx] = np.matmul(inv_rot_mat, dipvec[idx])\n",
    "\n",
    "    # Transfer position into leadfield space\n",
    "    dippos *= 1/scale\n",
    "\n",
    "    # to make it compatible with the lead field data\n",
    "    dippos = (dippos.astype('int')).astype('float')\n",
    "\n",
    "    # check if the depth restriction is exceeded\n",
    "    if max_depth is not None:\n",
    "        if dippos[2].any() < max_depth:\n",
    "            ## code to adjust depth\n",
    "            pass\n",
    "    \n",
    "    # Scale to dipole nAm magnitude, see value above\n",
    "    dipvec *= magnitude \n",
    "\n",
    "    return dippos, dipvec, rot_mat\n",
    "\n",
    "def trim_data(leadfield, vertices, normals):\n",
    "    \"\"\"\n",
    "    Set values outside of the leadfield as nan so that it is compatible for further calculation\n",
    "    inputs:\n",
    "        - leadfield, leadfield data in the form of [x,y,z,[vx,vy,vz],e]\n",
    "        - vertices, transformed data of the vertices (dippos)\n",
    "        - normals, transformed data of the normal vectors (dipvec)\n",
    "    outputs:\n",
    "        - trimmed dippos, dipvec\n",
    "    \"\"\"\n",
    "\n",
    "    dippos = np.copy(vertices)\n",
    "    dipvec = np.copy(normals)\n",
    "    # half of side length\n",
    "    len_half = leadfield.shape[0]//2\n",
    "\n",
    "    # um = 0\n",
    "    for idx in range(vertices.shape[0]):\n",
    "        if((np.abs(vertices[idx][0])>len_half) or (np.abs(vertices[idx][1])>len_half) or (vertices[idx][2])>len_half*2) or (vertices[idx][2]<0):\n",
    "            # print(\"yes\", um)\n",
    "            # um+=1\n",
    "            dippos[idx] = np.nan\n",
    "\n",
    "    return dippos, dipvec\n",
    "\n",
    "def calculate_voltage(fields, vertices, normals, vscale=10**6, montage = Montage, inter = False):\n",
    "    \"\"\"\n",
    "    Calculates voltage for each vertex\n",
    "    input: \n",
    "        - fields, leadfield data in the form of [x,y,z,[vx,vy,vz],e]\n",
    "        - vertices, transformed and trimmed data of the vertices (dippos)\n",
    "        - normals, transformed and trimmed data of the normal vectors (dipvec)\n",
    "        - vscale, to scale it to uV\n",
    "    output: \n",
    "        - opt_volt, a 1-D array that has the optimal voltage for each vertex\n",
    "    \"\"\"\n",
    "    global roi_vertices_weights, do_weights\n",
    "    \n",
    "    # make copies of data\n",
    "    dippos = np.copy(vertices)\n",
    "    dipvec = np.copy(normals)\n",
    "\n",
    "    # create a list of field vectors applicable to the surface, [vertex index, electrode #]\n",
    "    voltage = np.empty((dippos.shape[0], fields.shape[-1]))\n",
    "    voltage[:]=np.nan\n",
    "    opt_volt = np.empty(dippos.shape[0])\n",
    "    opt_volt[:]=np.nan\n",
    "    \n",
    "    # calculate voltage for each vertex\n",
    "    for idx in range(dippos.shape[0]): # for each dipole\n",
    "        for e in range(fields.shape[-1]): # for every electrode\n",
    "            if not np.any(np.isnan(dippos[idx])): # if not nan, calculate and modify entry\n",
    "                try:\n",
    "                    lead_field = fields[int(dippos[idx][0]+fields.shape[0]//2), int(dippos[idx][1]+fields.shape[0]//2), int(dippos[idx][2]), :, e]\n",
    "                    voltage[idx, e] = np.dot(lead_field, dipvec[idx])\n",
    "                except: # error in finding lead field\n",
    "                    voltage[idx, e] = np.nan\n",
    "        if (not np.all(np.isnan(voltage[idx,:]))) and (not montage): \n",
    "            opt_volt[idx] = np.nanmax(np.abs(np.copy(voltage[idx]))) # get the optimal voltage across all electrodes\n",
    "        elif (not np.all(np.isnan(voltage[idx,:]))) and montage and (not inter):\n",
    "            single = np.nanmax(np.abs(np.copy(voltage[idx]))) # get the optimal voltage across all electrodes\n",
    "            mont = np.nanmax(voltage[idx]) - np.nanmin(voltage[idx])\n",
    "            if mont > single: # Use montage if it's better\n",
    "                opt_volt[idx] = mont\n",
    "            else: # Keep zero reference if it's better\n",
    "                opt_volt[idx] = single\n",
    "        # elif (not np.all(np.isnan(voltage[idx,:]))) and montage and inter:\n",
    "    voltage *= vscale # (sources, electrodes) for one device\n",
    "    opt_volt *= vscale #scale it to get the list of voltages\n",
    "    snr_list = np.copy(opt_volt)/noise # get the list of snr values\n",
    "    info_cap = bandwidth*np.log2(1+snr_list) # get the list of information capacity\n",
    "    snr_list[np.isnan(snr_list)] = 0\n",
    "\n",
    "        # Scale with given weights for ROI subregions\n",
    "    if do_weights:\n",
    "        opt_volt = np.multiply(opt_volt,roi_vertices_weights)\n",
    "        snr_list = np.multiply(snr_list,roi_vertices_weights)\n",
    "        info_cap = np.multiply(info_cap,roi_vertices_weights)\n",
    "\n",
    "\n",
    "    return (opt_volt, snr_list, info_cap, voltage)\n",
    "\n",
    "def find_snr(devpos,value):\n",
    "    \"\"\"\n",
    "    Find the total SNR generated by N devices.\n",
    "    value: 0:voltage montaged :: 1:SNR :: 2:IC :: 3 or -1:raw voltage\n",
    "    \"\"\"\n",
    "    devpos = devpos.reshape((N, 6))\n",
    "    if value == 2: # IC\n",
    "        all_dev_volt = np.empty((recentered_roi.shape[0], N))\n",
    "    else:\n",
    "        all_dev_volt = np.empty((recentered_roi.shape[0], N, fields.shape[-1]))\n",
    "    for idx in range(len(devpos)):\n",
    "        dippos, dipvec, rotmat = transform_vectorspace(recentered_roi, roi_normals, devpos[idx])\n",
    "        dippos_adj, dipvec_adj = trim_data(fields, dippos, dipvec)\n",
    "        # Index decides volt, snr, IC, or voltage by(source,electrodes)\n",
    "        if value == 2: # IC\n",
    "            all_dev_volt[:, idx] = calculate_voltage(fields, dippos_adj, dipvec_adj, montage = Montage, inter = False)[value]\n",
    "        else:\n",
    "            all_dev_volt[:, idx,:] = calculate_voltage(fields, dippos_adj, dipvec_adj, montage = Montage, inter = False)[value]\n",
    "    \n",
    "    voltage = np.nanmax(np.nan_to_num(all_dev_volt, nan=0), axis=1)\n",
    "    total = np.nansum(voltage)\n",
    "    # print(\"total IC:\", total, \"Bits/s\")\n",
    "    return all_dev_volt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voltage calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjust fields to desired size ###\n",
    "disc_sensors = 64 # 64 or 128 for DiSc; sensors reduced evenly  in grid\n",
    "fields = field_reduce(fields,disc_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate virtual macros from DiSc ###\n",
    "if macros:\n",
    "    fields = virtual_macros(fields,virtual_rings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load brain and ROI ###\n",
    "# Values to collect\n",
    "brain_vals = []\n",
    "brain_vallen = [[],[],[]] # summative index for each start point\n",
    "brain_count = [0,0,0] # running index sum\n",
    "roi_vals = []\n",
    "roi_vallen = [[],[],[]]\n",
    "roi_count = [0,0,0]\n",
    "# Start indexing counts per file\n",
    "#for b in brain_data:\n",
    "#    brain_count.append(0)\n",
    "#for b in roi_data:\n",
    "#    roi_count.append(0)\n",
    "\n",
    "# Load each file\n",
    "for i,fb in enumerate(brain_data):\n",
    "    _, brain_faces, brain_vertices, brain_normals = obtain_data(brain_data[i], 'brain') # 'ans' or 'brain'\n",
    "    brain_vals.append([brain_faces,brain_vertices,brain_normals])\n",
    "    for k in range(3):\n",
    "        brain_vallen[k].append(brain_vals[i][k].shape[0] + brain_count[k])\n",
    "        brain_count[k] += brain_vals[i][k].shape[0]\n",
    "for i,fb in enumerate(roi_data):\n",
    "    _, roi_faces, roi_vertices, roi_normals = obtain_data(roi_data[i], 'auditory_roi') # 'ans' or 'auditory roi'\n",
    "    roi_vals.append([roi_faces,roi_vertices,roi_normals])\n",
    "    for k in range(3):\n",
    "        roi_vallen[k].append(roi_vals[i][k].shape[0] + roi_count[k])\n",
    "        roi_count[k] += roi_vals[i][k].shape[0]\n",
    "\n",
    "# Initialize final variables\n",
    "brain_faces = np.empty((brain_vallen[0][-1],3))\n",
    "brain_vertices = np.empty((brain_vallen[1][-1],3))\n",
    "brain_normals = np.empty((brain_vallen[2][-1],3))\n",
    "roi_faces = np.empty((roi_vallen[0][-1],3))\n",
    "roi_vertices = np.empty((roi_vallen[1][-1],3))\n",
    "roi_normals = np.empty((roi_vallen[2][-1],3))\n",
    "roi_vertices_weights = np.zeros((roi_vallen[1][-1],)) # matching index weights\n",
    "\n",
    "# Merge file variables\n",
    "for i in range(len(brain_data)): # i for each file index\n",
    "    if i == 0: # first file starts from zero in lists\n",
    "        brain_faces[0:brain_vallen[0][i]] = brain_vals[i][0]\n",
    "        brain_vertices[0:brain_vallen[1][i]] = brain_vals[i][1]\n",
    "        brain_normals[0:brain_vallen[2][i]] = brain_vals[i][2]\n",
    "    else:\n",
    "        brain_faces[brain_vallen[0][i-1]:brain_vallen[0][i]] = brain_vals[i][0]\n",
    "        brain_vertices[brain_vallen[1][i-1]:brain_vallen[1][i]] = brain_vals[i][1]\n",
    "        brain_normals[brain_vallen[2][i-1]:brain_vallen[2][i]] = brain_vals[i][2]\n",
    "for i in range(len(roi_data)): # i for each roi file\n",
    "    if i == 0: # first file starts from zero in lists\n",
    "        roi_faces[0:roi_vallen[0][i]] = roi_vals[i][0]\n",
    "        roi_vertices[0:roi_vallen[1][i]] = roi_vals[i][1]\n",
    "        roi_normals[0:roi_vallen[2][i]] = roi_vals[i][2]\n",
    "\n",
    "        roi_vertices_weights[0:roi_vallen[1][i]] = np.repeat(roi_weights[i],roi_vallen[1][i])\n",
    "    else:\n",
    "        roi_faces[roi_vallen[0][i-1]:roi_vallen[0][i]] = roi_vals[i][0]\n",
    "        roi_vertices[roi_vallen[1][i-1]:roi_vallen[1][i]] = roi_vals[i][1]\n",
    "        roi_normals[roi_vallen[2][i-1]:roi_vallen[2][i]] = roi_vals[i][2]\n",
    "        roi_vertices_weights[roi_vallen[1][i-1]:roi_vallen[1][i]] += np.repeat(roi_weights[i],(roi_vallen[1][i] - roi_vallen[1][i-1]))\n",
    "\n",
    "# Recenter total ROI based on total brain\n",
    "recentered_roi = recenter(roi_vertices, brain_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate voltages for best position set ###\n",
    "voltages = []  # Must be flexible list for different sensor counts\n",
    "infocap = [] # Collect IC values\n",
    "sensor_count = fields.shape[-1]\n",
    "\n",
    "# Manual trajectory injection\n",
    "manual = False\n",
    "\n",
    "# Manual trajectory demo\n",
    "manual_trajectories = [ # Each inside list is (N*6,) for coordinates for N devices\n",
    "    np.array([0, 0, 5, 0, 0, 180]),\n",
    "    np.array([0, 3, 4.5, 0, 0, 180,\n",
    "              0, -3, 6.5, 0, 0, 180]),\n",
    "    np.array([-.5, -3, 6, 0, 0, 180,\n",
    "              -.5, 3, 4.5, 0, 0, 180,\n",
    "              2, 0, 5, 0, 0, 180])\n",
    "]\n",
    "\n",
    "if manual: # Rebuild device count list\n",
    "    N_file = []\n",
    "    for t in manual_trajectories:\n",
    "        N_file.append(t.shape[0]//6)\n",
    "        if t.shape[0]%6 != 0:\n",
    "            print(\"Error: Manual trajectory incorrect.\")\n",
    "            break\n",
    "\n",
    "# Main loop\n",
    "for i,N in enumerate(N_file): # each i file of N devices\n",
    "    if manual:\n",
    "        voltages.append(find_snr(manual_trajectories[i],-1))\n",
    "        infocap.append(find_snr(manual_trajectories[i],2))\n",
    "    else:\n",
    "        voltages.append(find_snr(best[i],-1))\n",
    "        infocap.append(find_snr(best[i],2))\n",
    "    voltages[i] = voltages[i].reshape(-1,N*sensor_count)\n",
    "    print(f\"Done with {N} devices with voltage array shape: {voltages[i].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess voltage and IC\n",
    "Not used in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(infocap[0].shape)\n",
    "print(np.count_nonzero(np.isnan(infocap[0])))\n",
    "print(np.count_nonzero(np.isnan(np.sum(voltages[0],axis=1))))\n",
    "print(np.where(~np.isnan(infocap[0]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot IC distribution for one device by distance of sources ###\n",
    "# Avoid NaN indices that are not observed by the dataset\n",
    "notnan = np.where(~np.isnan(infocap[0]))[0] # safe indices\n",
    "\n",
    "# Distance to each point from the device assigned position; matched indices to voltage\n",
    "distance = recentered_roi - best[0][0:3]\n",
    "distance = np.sqrt(np.sum(distance**2,axis=-1))[notnan]\n",
    "\n",
    "# Plot source distances\n",
    "plt.hist(distance,bins=50, color='lightblue',edgecolor='black')\n",
    "plt.xlim([0,30])\n",
    "plt.xlabel('Device-Source Distance (mm)')\n",
    "plt.ylabel('Source Count')\n",
    "plt.savefig(path.join(out,f\"8_Distance-distribution.png\"),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot IC voilin for DiSc and vSEEG at varying range\n",
    "bins = [0,7.5,15,22.5,30] # mm distance groups\n",
    "binned_ic = [[] for _ in range(len(bins)-1)]\n",
    "positions = []\n",
    "for i in (range(len(bins)-1)):\n",
    "    positions.append((bins[i]+bins[i+1])/2)\n",
    "positions = np.asarray(positions)\n",
    "\n",
    "for dist, ic in zip(distance,infocap[0][notnan]):\n",
    "    bin_idx = np.digitize(dist,bins) - 1\n",
    "    if 0<=bin_idx<(len(bins)-1):\n",
    "        binned_ic[bin_idx].append(ic[0])\n",
    "\n",
    "plt.violinplot(binned_ic,positions=positions,showmeans=True,widths=(bins[1]-bins[0])/1.5)\n",
    "plt.xticks(bins)\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlabel('Binned Distance (mm)')\n",
    "plt.ylabel('Info. Cap. Distribution')\n",
    "plt.savefig(path.join(out,f\"8_IC-distribution-{voltages[0].shape[-1]}e.png\"),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assess voltages ###\n",
    "print(\"Voltage summary\")\n",
    "for i in range(len(N_file)):\n",
    "    print(\"N:\",N_file[i])\n",
    "    print(\"Mean:\",np.nanmean(voltages[i]))\n",
    "    #print(\"Max:\",np.nanmax(voltages[i]))\n",
    "    #print(\"Min:\",np.nanmin(voltages[i]))\n",
    "    print(\"STD:\",np.nanstd(voltages[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assess Information Capacity ###\n",
    "print(\"Information capacity summary\")\n",
    "for i in range(len(N_file)):\n",
    "    print(\"N:\",N_file[i])\n",
    "    print(\"Mean:\",np.nanmean(np.nanmax(infocap[i],axis=1)))\n",
    "    #print(\"Max:\",np.nanmax(np.nanmax(infocap[i],axis=1)))\n",
    "    #print(\"Min:\",np.nanmin(np.nanmax(infocap[i],axis=1)))\n",
    "    print(\"STD:\",np.nanstd(np.nanmax(infocap[i],axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEPIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEPIO train/test with variable replicates ###\n",
    "MC_cycles = 1 # How many Monte-Carlo cycles to complete and average\n",
    "rep_per_max = 1 # (INT) replicates per max sensors\n",
    "sensor_div = 16 # Sensor testing granularity (defines sensor_range steps)\n",
    "noise_mult = 10. # Boost noise uniformly\n",
    "totalcoefs = [] # Coefficients per test set\n",
    "totalacc = [] # Test acc (sensor space)\n",
    "spatialacc = [] # Test acc (source space)\n",
    "sensor_list = [] # list of sensor_range\n",
    "\n",
    "for i, N in enumerate(N_file):\n",
    "    print(f\"Starting loop {i+1} with {N} devices.\")\n",
    "    MCcoefs, MCaccs, MCSaccs, sensor_range = sepio.mc_train(X=voltages[i],y=None,Xt=None,yt=None,sensor_div=sensor_div,\n",
    "    MCcount=MC_cycles,noise=noise*noise_mult,replicates=rep_per_max*voltages[i].shape[-1],nbasis=20,spatial=False,l1=0.001,rep_subdiv=8)\n",
    "    totalcoefs.append(MCcoefs)\n",
    "    totalacc.append(MCaccs)\n",
    "    spatialacc.append(MCSaccs)\n",
    "    sensor_list.append(sensor_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check top-N sensors alone\n",
    "if False:\n",
    "    topN = 16\n",
    "    dataset = 0 # index for file\n",
    "    rep_per_max = 1\n",
    "    sensor_div = 2 # Sensor testing granularity (defines sensor_range steps)\n",
    "    sensors = np.flip(np.argsort(np.mean(np.abs(totalcoefs[dataset]),axis=1)))[:16]\n",
    "    TNcoefs, TNaccs, TNSaccs, TNsensor_range = sepio.mc_train(X=voltages[dataset][:,sensors],y=None,Xt=None,yt=None,sensor_div=sensor_div,\n",
    "    MCcount=1,noise=4.1,replicates=rep_per_max*voltages[dataset][:,sensors].shape[-1],nbasis=20,spatial=False,l1=0.001,rep_subdiv=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmin(np.array(MCSaccs)))\n",
    "print(np.nanmean(np.array(MCSaccs)),np.std(np.nan_to_num(np.array(MCSaccs))))\n",
    "print(np.nanmax(np.array(MCSaccs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(totalacc)\n",
    "for n in range(len(totalacc)):\n",
    "    print(totalacc[n][-1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting accuracies for each device set ###\n",
    "for i,N in enumerate(N_file):\n",
    "    if i < len(totalacc):\n",
    "        if N == 1:\n",
    "            plt.plot(np.arange(0,sensor_count,sensor_div)+sensor_div,100*totalacc[i],label=\"1 Device\")\n",
    "        else:\n",
    "            plt.plot(np.arange(0,sensor_count*N,sensor_div)+sensor_div,100*totalacc[i],label=f\"{N} Devices\")\n",
    "\n",
    "plt.ylabel(f\"% Accuracy for {voltages[0].shape[0]} classes\")\n",
    "plt.xlabel(\"Sensor count\")\n",
    "plt.title(f\"SEPIO classification accuracy\\n({data[0]['type'][0]})\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.savefig(path.join(out,f\"8_SEPIO_total-{data[0]['type'][0]}.png\"),dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot sensorXdevice for different levels of accuracy ###\n",
    "# Use totalacc (N, acc[i] for i at sensors*16)\n",
    "\n",
    "# Assign plot values\n",
    "xs = []\n",
    "ys = []\n",
    "zs = []\n",
    "for n in range(len(totalacc)): # k is acc\n",
    "    # n+1 for each device count\n",
    "    x = []\n",
    "    y = []\n",
    "    for s in range(totalacc[n].size):\n",
    "        # (s+1)*16 for each sensor count\n",
    "        xs.append((s+1)*sensor_div)\n",
    "        ys.append(N_file[n])\n",
    "        zs.append(totalacc[n][s]*100)\n",
    "\n",
    "# Plot settings\n",
    "plot_divs = 18 # number of plot divisions\n",
    "#plt.tricontour(xs,ys,zs,levels=plot_divs,colors='k') # Adds lines between regions\n",
    "pt = plt.tricontourf(xs,ys,zs,levels=plot_divs,cmap=\"RdBu_r\") # colors in regions\n",
    "\n",
    "plt.yticks(N_file[:len(totalacc)])\n",
    "plt.xlim([0,max(N_file[:len(totalacc)])*fields.shape[-1]])\n",
    "plt.ylabel(\"Device count\")\n",
    "plt.xlabel(\"Sensor count\")\n",
    "plt.title(f\"SEPIO by device and sensor count\\n({data[0]['type'][0]})\")\n",
    "cb = plt.colorbar(pt)\n",
    "cb.set_label(\"% Accuracy\")\n",
    "plt.savefig(path.join(out,f\"8_SEPIO_SxN-{data[0]['type'][0]}.png\"),dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
